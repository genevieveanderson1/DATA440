# -*- coding: utf-8 -*-
"""DATA440_FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zKl_aKSX3y6_6dm-Ul0rUJ3cgcKP3NaJ

## Final Project

The goal of my project is to try to predict the possibility death from heart failure based on the observations on this data set.

Uploading the data
"""

from google.colab import files
 
 
uploaded = files.upload()

import pandas as pd
import io
 
df = pd.read_csv(io.BytesIO(uploaded['heart_failure_clinical_records_dataset.csv']))
df.head()

"""Cleaning the data"""

# Checking for null values
df.isnull().sum()

df.info()

"""Creating the target"""

X=df.drop('DEATH_EVENT',axis=1)
y=df['DEATH_EVENT']

# Looking at the distribution of the target
import seaborn as sns
sns.set_style('whitegrid')
sns.countplot(x='DEATH_EVENT',data=df);

"""Splitting the data"""

from sklearn.model_selection import train_test_split

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

"""Gaussian Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

Gnb = GaussianNB()
Gnb.fit(X_train, y_train)

import scipy.stats as st
import numpy as np

# parameters
dflist=[]
class_values = df['DEATH_EVENT'].unique()
features = df.drop('DEATH_EVENT', axis=1).columns

for i, class_value in enumerate(class_values):
   means = Gnb.theta_
   stdevs = np.sqrt(Gnb.var_)
   pdfs =st.norm.pdf(means,stdevs)
   Gaussianparams = {'mean':means[i,:],'stdev':stdevs[i,:], 'Conditional Density':pdfs[i,:] }
   dflist.append(pd.DataFrame(data = Gaussianparams, index=features))

for i,df in enumerate(dflist):
  print('DEATH_EVENT:',class_values[i])
  print(df,'\n')

from sklearn import metrics

y_pred = Gnb.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score 
from sklearn.metrics import confusion_matrix,roc_curve, auc

# report the predctive performance metrics
# evaluate predictions
accuracy = accuracy_score(y_test, y_pred)

print("\n")
print(f"Accuracy.........: {accuracy * 100.0:.4f}")

cm = confusion_matrix(y_test, y_pred)
print(f"Confusion matrix.:\n {cm} \n")

#This is nicer
sns.heatmap(cm, annot=True,fmt='d',cbar=False);

from sklearn.metrics import classification_report

# Classification report
print(classification_report(y_test,y_pred))

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(random_state=0, solver='liblinear',multi_class='ovr')
clf.fit(X_train,y_train)

# Predictions
y_pred = clf.predict(X_test)

# Probabilities for positive class
probs = clf.predict_proba(X_test)[:,1]
probs[:10]

# Note that we multiply by one to report numbers.If not the expression reports True or False.
1 * (probs >0.5)[0:20]

# Confusion matrix

print(confusion_matrix(y_test,y_pred))

from sklearn.metrics import classification_report

# Classification report
print(classification_report(y_test,y_pred))

from sklearn.metrics import accuracy_score

# Overall accuracy
print(accuracy_score(y_test,y_pred))

"""Random Forest"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import warnings
import pandas as pd
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')

!pip install scikit-optimize

import time
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier() #n_estimators is 100 by default

start = time.time()
rfc.fit(X_train, y_train)
end = time.time()
elapsed_time_notuning = (end-start)
print(f"Elapsed time = {elapsed_time_notuning:.2f}")
ypred_notuning = rfc.predict(X_test)

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from skopt import BayesSearchCV

# define sample space
param_grid = {
    'n_estimators': [100,200, 300, 400,500],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3, 4, 5]
    }

scoring_metric='accuracy'

randomized = RandomizedSearchCV(estimator=rfc,
                                param_distributions=param_grid,
                                scoring=scoring_metric,
                                cv=5,
                                n_jobs=-1,
                                verbose=20,
                                n_iter=50)

start = time.time()
randomized.fit(X_train, y_train)
end = time.time()
elapsed_time_random = (end-start)
print(f"Elapsed time = {elapsed_time_random:.2f}")
ypred_random = randomized.predict(X_test)

from sklearn.metrics import accuracy_score

# Overall accuracy
print(accuracy_score(y_test,ypred_notuning))
print(accuracy_score(y_test,ypred_random))

# Classification report
from sklearn import metrics
print('No tuning:\n')
print(metrics.classification_report(ypred_notuning, y_test),"\n")
print(metrics.confusion_matrix(ypred_notuning, y_test),"\n\n")

print('Randomized:\n')
print(metrics.classification_report(ypred_random, y_test),"\n")
print(metrics.confusion_matrix(ypred_random, y_test),"\n\n")

"""XG Boost"""

import time
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from xgboost import XGBClassifier

xgb = XGBClassifier(learning_rate=0.02, 
                    n_estimators=600, 
                    objective='binary:logistic',
                    silent=True, 
                    nthread=1)

# A parameter grid for XGBoost
params = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [3, 4, 5]
        }

def searchCV (search_type ='random', folds=10, param_comb = 20):
   
  if search_type =='random':
    search_ = RandomizedSearchCV(xgb, 
                                       param_distributions=params, 
                                       n_iter=param_comb, 
                                       scoring='roc_auc', 
                                       n_jobs=-1, 
                                       cv=folds,
                                       #cv=skf.split(X_train,y_train), 
                                       verbose=20, 
                                       random_state=42)
  elif search_type =='grid':
    search_ = GridSearchCV(estimator=xgb,
                                param_grid=params,
                                scoring='roc_auc',
                                n_jobs=-1,
                                cv=folds,
                                #cv=skf.split(X_train,y_train),
                                verbose=20) 
  return search_

# Tuning
start_time = time.time()
folds = 10
param_comb = 20
srchCV = searchCV (search_type ='random', folds=10, param_comb = 20)
srchCV.fit(X_train, y_train)
#random_search.fit(X_train, y_train)
print(f"Execution time:{(time.time() - start_time):.2f} seg ---")

print('\n Best estimator:')
print(srchCV.best_estimator_)
print(f'\n Best AUC for {folds:d}-fold search:')
print(srchCV.best_score_ )
print('\n Best hyperparameters:')
print(srchCV.best_params_)
results = pd.DataFrame(srchCV.cv_results_)
results.to_csv('xgb-random-grid-search-results-01.csv', index=False)

#get predictions on the test set
predictions = srchCV.predict(X_test)

# get probabilities for positive class
probs = srchCV.predict_proba(X_test)[:,1]

# Report the predictive performance metrics
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
precision=precision_score(y_test, predictions, )
recall=recall_score(y_test, predictions)
specificity=recall_score(y_test, predictions, pos_label=0)
roc = roc_auc_score(y_test, probs)

print("\n")
print(f"Accuracy.........: {accuracy * 100.0:.4f}")
print(f"Precision........: {precision *100:.4f}")
print(f"Recall...........: {recall * 100:.4f}")
print(f"FP Rate...........:{(1-specificity) * 100:.4f}")
print(f"ROC AUC (probs)..: {roc:.6f}")

cm = confusion_matrix(y_test, predictions)
print(f"Confusion matrix.:\n {cm}")

"""Modifying the dataset to try to get better accuracy"""

df1 = pd.read_csv(io.BytesIO(uploaded['heart_failure_clinical_records_dataset.csv']))
df1.head()

df2 = df1.drop('age', axis = 1)
df3 = df2.drop('anaemia', axis = 1)
df4 = df3.drop('creatinine_phosphokinase', axis = 1)
df5 = df4.drop('ejection_fraction', axis = 1)
df6 = df5.drop('serum_creatinine', axis = 1)
df7 = df6.drop('serum_sodium', axis = 1)
df8 = df7.drop('sex', axis = 1)
df9 = df8.drop('time', axis = 1)
df10 = df9.drop('platelets', axis = 1)
df10.head()

X=df10.drop('DEATH_EVENT',axis=1)
y=df10['DEATH_EVENT']

from sklearn.model_selection import train_test_split

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB().fit(X_train, y_train)

# get probabilities for positive class
probs = model.predict_proba(X_test)[:,1]

# get predictions
predictions = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score,auc, f1_score
from sklearn.metrics import confusion_matrix,roc_curve, precision_recall_curve

# Report the predctive performance metrics
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
precision=precision_score(y_test, predictions, average='micro')
recall=recall_score(y_test, predictions, average = 'micro')
specificity=recall_score(y_test, predictions, pos_label=0, average = 'micro')
f1=f1_score(y_test, predictions, average = 'micro')

print("\n")
print(f"Accuracy.........: {accuracy * 100.0:.4f}")
print(f"Precision........: {precision *100:.4f}")
print(f"Recall...........: {recall * 100:.4f}")
print(f"FP Rate...........:{(1-specificity) * 100:.4f}")
print(f"F1 score.........:{(f1):.4f}")

from sklearn.metrics import classification_report

# Classification report
print(classification_report(y_test,predictions))